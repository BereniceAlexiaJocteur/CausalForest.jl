{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a16fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CausalForest\n",
    "using RCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f6657cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: RCall.jl: Le chargement a nécessité le package : quantreg\n",
      "│ Error: le chargement du package ou de l'espace de noms a échoué pour ‘quantreg’ in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) :\n",
      "│ le package ‘Matrix’ 1.4-1 est déjà chargé, mais >= 1.4.2 est requis\n",
      "│ Le chargement a nécessité le package : glmnet\n",
      "│ Le chargement a nécessité le package : Matrix\n",
      "│ Failed with error:  ‘Le package ‘Matrix’ version 1.4.1 ne peut pas être chargé :\n",
      "│ Error in unloadNamespace(package) : l'espace de noms ‘Matrix’ est importé par ‘survival’, ‘xgboost’, ‘grf’, ‘ranger’, ‘recipes’ et ne peut, donc, pas être déchargé\n",
      "│ ’\n",
      "│ Le chargement a nécessité le package : multcomp\n",
      "│ Le chargement a nécessité le package : survival\n",
      "│ Avis dans FUN(X[[i]], ...) :\n",
      "│   redémarrage de l'évaluation d'une promesse interrompue\n",
      "│ Avis dans FUN(X[[i]], ...) : unknown type in R_decompress3\n",
      "│ Failed with error:  ‘lazy-load database '/Library/Frameworks/R.framework/Versions/4.2/Resources/library/rpart/data/Rdata.rdb' is corrupt’\n",
      "│ Le chargement a nécessité le package : bartMachine\n",
      "│ Le chargement a nécessité le package : rJava\n",
      "│ Error: le chargement du package ou de l'espace de noms a échoué pour ‘rJava’ :\n",
      "│ .onLoad a échoué dans loadNamespace() pour 'rJava', détails :\n",
      "│   appel : dyn.load(jvm, FALSE)\n",
      "│   erreur : impossible de charger l'objet partagé '/Library/Java/JavaVirtualMachines/temurin-19.jdk/Contents/Home/lib/server/libjvm.dylib':\n",
      "│   dlopen(/Library/Java/JavaVirtualMachines/temurin-19.jdk/Contents/Home/lib/server/libjvm.dylib, 0x000A): tried: '/Library/Java/JavaVirtualMachines/temurin-19.jdk/Contents/Home/lib/server/libjvm.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n",
      "│ Failed with error:  ‘le package ‘rJava’ ne peut être chargé’\n",
      "└ @ RCall /Users/jocteur/.julia/packages/RCall/6kphM/src/io.jl:172\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "REvalError: Erreur dans get(library$library$predAlgorithm[s], envir = env) : \n  objet 'SL.nnet_1' introuvable\nAppels : SuperLearner -> lapply -> FUN -> get",
     "output_type": "error",
     "traceback": [
      "REvalError: Erreur dans get(library$library$predAlgorithm[s], envir = env) : \n  objet 'SL.nnet_1' introuvable\nAppels : SuperLearner -> lapply -> FUN -> get",
      "",
      "Stacktrace:",
      " [1] handle_eval_stderr(; as_warning::Bool)",
      "   @ RCall ~/.julia/packages/RCall/6kphM/src/io.jl:174",
      " [2] reval_p(expr::Ptr{LangSxp}, env::Ptr{EnvSxp})",
      "   @ RCall ~/.julia/packages/RCall/6kphM/src/eval.jl:103",
      " [3] reval_p(expr::Ptr{RCall.ExprSxp}, env::Ptr{EnvSxp})",
      "   @ RCall ~/.julia/packages/RCall/6kphM/src/eval.jl:119",
      " [4] reval(str::String, env::RObject{EnvSxp})",
      "   @ RCall ~/.julia/packages/RCall/6kphM/src/eval.jl:136",
      " [5] top-level scope",
      "   @ ~/.julia/packages/RCall/6kphM/src/macros.jl:71",
      " [6] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [7] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "R\"\"\"\n",
    "vec.pac= c(\"foreign\",\"quantreg\", \"gbm\", \"glmnet\",\n",
    "           \"MASS\", \"rpart\", \"nnet\", \"matrixStats\",\n",
    "           \"xtable\", \"readstata13\",\"grf\",\"remotes\",\n",
    "           \"caret\",  \"multcomp\",\"cowplot\",\"SuperLearner\",\n",
    "           \"ranger\",\"reshape2\",\"gridExtra\",\"bartCause\",\"xgboost\",\"bartMachine\",\"nnet\")\n",
    "\n",
    "lapply(vec.pac, require, character.only = TRUE) \n",
    "#lapply(vec.pac, library, character.only = TRUE) \n",
    "\n",
    "# Read the microcredit data\n",
    "data        <- read.dta13(\"data_rep.dta\")\n",
    "\n",
    "\n",
    "####################################### Inputs  #######################################\n",
    "\n",
    "data$y <- data$loansamt_total\n",
    "data$loansamt_total <- NULL\n",
    "\n",
    "data$d <- data$treatment\n",
    "data$treatment <- NULL\n",
    "# \"loansamt_total\"     vector of outcome variables\n",
    "# \"treatment\"   vector of treatment variables\n",
    "\n",
    "\n",
    "\n",
    "# create a vector of control variables\n",
    "covariates     <- c(\"members_resid_bl\", \"nadults_resid_bl\", \"head_age_bl\", \"act_livestock_bl\", \"act_business_bl\", \n",
    "                    \"borrowed_total_bl\", \"members_resid_d_bl\", \"nadults_resid_d_bl\", \"head_age_d_bl\", \"act_livestock_d_bl\", \n",
    "                    \"act_business_d_bl\", \"borrowed_total_d_bl\", \"ccm_resp_activ\", \"other_resp_activ\", \"ccm_resp_activ_d\", \n",
    "                    \"other_resp_activ_d\", \"head_educ_1\", \"nmember_age6_16\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "\n",
    "data <- data[,c(\"y\", \"d\",covariates)]\n",
    "# erase some missing values \n",
    "data <- data[complete.cases(data),]\n",
    "data$ID <- c(1:nrow(data))\n",
    "\n",
    "B <- 400 # Number of Bootstrap repetitions\n",
    "\n",
    "#Learner Library:\n",
    "\n",
    "#SL.ranger_td = create.Learner(\"SL.ranger\", params = list(num.trees = 1000, min.node.size = 10))\n",
    "learners <- c( \"SL.nnet_1\",\"SL.ranger_1\")\n",
    "\n",
    "#CV Control for the SuperLearner\n",
    "control <- SuperLearner.CV.control(V=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a matrix to store the CATE results from each method \n",
    "results_cate_DR <- matrix(0,nrow(data),B)\n",
    "results_cate_R <- matrix(0,nrow(data),B)\n",
    "results_cate_T <- matrix(0,nrow(data),B)\n",
    "results_cate_X <- matrix(0,nrow(data),B)\n",
    "cate_CBART <- as.data.frame(matrix(0,nrow(data),3))\n",
    "cate_CForest <- as.data.frame(matrix(0,nrow(data),3))                   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "createbootstrappedData <- function(df_boot) {\n",
    "  \n",
    "  smpl_0 <- sample((1:nrow(df_boot))[df_boot$d == 0],\n",
    "                   replace = TRUE,\n",
    "                   size = sum(1 - df_boot$d))\n",
    "  smpl_1 <- sample((1:nrow(df_boot))[df_boot$d == 1],\n",
    "                   replace = TRUE,\n",
    "                   size = sum(df_boot$d))\n",
    "  smpl <- sample(c(smpl_0, smpl_1))\n",
    "  \n",
    "  return(df_boot[smpl,])\n",
    "}\n",
    "\n",
    "set.seed(1234)\n",
    "folds <- createFolds(data$d,k=5)\n",
    "\n",
    "for(f in 1:(length(folds))){\n",
    "  \n",
    "  if(f == 1){\n",
    "    data1 <- data[c(folds[[5]],folds[[2]],folds[[3]],folds[[4]]),]\n",
    "    df_main <- data[folds[[1]],]\n",
    "  } \n",
    "  if(f == 2){\n",
    "    data1 <- data[c(folds[[1]],folds[[5]],folds[[3]],folds[[4]]),]\n",
    "    df_main <- data[folds[[2]],]\n",
    "  } \n",
    "  \n",
    "  if(f == 3){\n",
    "    data1 <- data[c(folds[[1]],folds[[2]],folds[[5]],folds[[4]]),]\n",
    "    df_main <- data[folds[[3]],]\n",
    "  } \n",
    "  \n",
    "  if(f == 4){\n",
    "    data1 <- data[c(folds[[1]],folds[[2]],folds[[3]],folds[[5]]),]\n",
    "    df_main <- data[folds[[4]],]\n",
    "  } \n",
    "  \n",
    "  if(f == 5){\n",
    "    data1 <- data[c(folds[[1]],folds[[2]],folds[[3]],folds[[4]]),]\n",
    "    df_main <- data[folds[[5]],]\n",
    "  } \n",
    "  \n",
    "  \n",
    "  \n",
    "  for(b in 1:B){\n",
    "    \n",
    "    set.seed(1011+b)\n",
    "    \n",
    "    ### Apply the meta-learners with Bootstrapping  \n",
    "    \n",
    "    df_aux <- createbootstrappedData(data1)\n",
    "    \n",
    "    ## DR-learner \n",
    "    # Train a classification model to get the propensity scores\n",
    "    p_mod <- SuperLearner(Y = df_aux$d, X = df_aux[,covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                          verbose = FALSE, method = \"method.NNLS\", family = binomial(),cvControl = control)\n",
    "    \n",
    "    p_hat <- p_mod$SL.predict\n",
    "    p_hat = ifelse(p_hat<0.025, 0.025, ifelse(p_hat>.975,.975, p_hat)) # Overlap bounding\n",
    "    \n",
    "    # Prop-Score for df_main\n",
    "    #p_hat_main <- predict(p_mod,df_main[,covariates])$pred\n",
    "    \n",
    "    # Split the training data into treatment and control observations\n",
    "    aux_1 <- df_aux[which(df_aux$d==1),]\n",
    "    aux_0 <- df_aux[which(df_aux$d==0),]\n",
    "    \n",
    "    # Train a regression model for the treatment observations\n",
    "    m1_mod <- SuperLearner(Y = aux_1$y, X = aux_1[,covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                           verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "    \n",
    "    m1_hat <- m1_mod$SL.predict\n",
    "    \n",
    "    # Train a regression model for the control observations\n",
    "    m0_mod <- SuperLearner(Y = aux_0$y, X = aux_0[,covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                           verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "    \n",
    "    m0_hat <- m0_mod$SL.predict\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Apply the doubly-robust estimator \n",
    "    y_mo <- (m1_hat - m0_hat) + ((df_main$d*(df_main$y -m1_hat))/p_hat) - ((1-df_main$d)*(df_main$y - m0_hat)/(1-p_hat))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    a  <- tryCatch({\n",
    "      dr_mod <- SuperLearner(Y = y_mo, X = df_main[,covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                             verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "      \n",
    "      score_dr <- dr_mod$SL.predict\n",
    "      a <- score_dr\n",
    "      \n",
    "      \n",
    "    },error=function(e){\n",
    "      \n",
    "      mean_score <- mean(y_mo)\n",
    "      score_dr <- rep.int(mean_score, times = nrow(df_main))\n",
    "      a <- score_dr\n",
    "      return(a)\n",
    "    })\n",
    "    \n",
    "    score_dr <- a\n",
    "    \n",
    "    ######################\n",
    "    \n",
    "    results_cate_DR[df_main$ID,b] <- score_dr\n",
    "    results_cate_T[df_main$ID,b] <- (predict(m1_mod,df_main[,covariates])$pred - predict(m0_mod,df_main[,covariates])$pred)\n",
    "    \n",
    "    ### R-learner \n",
    "    \n",
    "    # Train a regression model \n",
    "    m_mod <- SuperLearner(Y = df_aux$y, X = df_aux[,covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                          verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "    \n",
    "    m_hat <- m_mod$SL.predict\n",
    "    \n",
    "    # Apply the R-learner (residual-on-residual approach)\n",
    "    y_tilde = df_main$y - m_hat\n",
    "    w_tilde = df_main$d - p_hat\n",
    "    pseudo_outcome = y_tilde/w_tilde\n",
    "    \n",
    "    weights = w_tilde^2\n",
    "    \n",
    "    \n",
    "    a  <- tryCatch({\n",
    "      R_mod <- SuperLearner(Y = pseudo_outcome, X = df_main[,covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                            verbose = FALSE, method = \"method.NNLS\",obsWeights = weights[,1],cvControl = control)\n",
    "      score_R <- R_mod$SL.predict\n",
    "      a <- score_R\n",
    "      \n",
    "      \n",
    "    },error=function(e){\n",
    "      \n",
    "      mean_score <- weighted.mean(pseudo_outcome, w = weights)\n",
    "      score_R <- rep.int(mean_score, times = nrow(df_main))\n",
    "      a <- score_R\n",
    "      return(a)\n",
    "    })\n",
    "    \n",
    "    score_R <- a\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    results_cate_R[df_main$ID,b] <- score_R\n",
    "    \n",
    "    ###  X-learner\n",
    "    \n",
    "    tau1 <- df_main[which(df_main$d==1),\"y\"] - m0_hat[which(df_main$d==1),]\n",
    "    \n",
    "    tau0 <- m1_hat[which(df_main$d==0),]  -  df_main[which(df_main$d==0),\"y\"]\n",
    "    \n",
    "    \n",
    "    a1  <- tryCatch({\n",
    "      tau1_mod <- SuperLearner(Y = tau1, X = df_main[which(df_main$d==1),covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                               verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "      \n",
    "      score_tau1 <- tau1_mod$SL.predict\n",
    "      a1 <- score_tau1\n",
    "      \n",
    "      \n",
    "    },error=function(e){\n",
    "      \n",
    "      mean_score <- mean(tau1)\n",
    "      score_tau1 <- rep.int(mean_score, times = nrow(df_main))\n",
    "      a1 <- score_tau1\n",
    "      return(a1)\n",
    "    })\n",
    "    \n",
    "    score_tau1 <- a1\n",
    "    \n",
    "    a0  <- tryCatch({\n",
    "      tau0_mod <- SuperLearner(Y =tau0, X = df_main[which(df_main$d==0),covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                               verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "      \n",
    "      score_tau0 <- tau0_mod$SL.predict\n",
    "      a0 <- score_tau0\n",
    "      \n",
    "      \n",
    "    },error=function(e){\n",
    "      \n",
    "      mean_score <- mean(tau0)\n",
    "      score_tau0 <- rep.int(mean_score, times = nrow(df_main))\n",
    "      a0 <- score_tau0\n",
    "      return(a0)\n",
    "    })\n",
    "    \n",
    "    score_tau0 <- a0\n",
    "    \n",
    "    \n",
    "    \n",
    "    score_X <- p_hat*score_tau0 + (1-p_hat)*score_tau1\n",
    "    \n",
    "    results_cate_X[df_main$ID,b] <- score_X\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cat(\"This is Iteration: \", b, \"out of\", B,\"\\n\")\n",
    "  }\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  ### Causal BART\n",
    "  fit_bart <- bartc(data1$y,data1$d,data1[,covariates],keepTrees = TRUE,ntree=2000)\n",
    "  score_bart <- predict(fit_bart,newdata = df_main,type=\"icate\")\n",
    "  score_bart_m <- apply(score_bart,2,mean)\n",
    "  ite.sd <- apply(score_bart, 2, sd)\n",
    "  ite.lb <- score_bart_m - 1.96 * ite.sd\n",
    "  ite.ub <- score_bart_m + 1.96 * ite.sd\n",
    "  \n",
    "  \n",
    "  cate_CBART[df_main$ID,] <- as.data.frame(cbind(score_bart_m,ite.lb,ite.ub))\n",
    "  colnames(cate_CBART) <- c(\"pred\",\"X5.\",\"X95.\")\n",
    "  \n",
    "  ### Causal Forest\n",
    "  \n",
    "  forest.D <- regression_forest(data1[,covariates], data1$d, tune.parameters = \"all\")\n",
    "  W.hat <- predict(forest.D)$predictions\n",
    "  \n",
    "  forest.Y <- regression_forest(data1[,covariates], data1$y, tune.parameters = \"all\")\n",
    "  Y.hat <- predict(forest.Y)$predictions\n",
    "  \n",
    "  tau.forest <- causal_forest(data1[,covariates], data1$y, data1$d,tune.parameters = \"all\",num.trees = 8000,\n",
    "                              min.node.size=5,W.hat = W.hat, Y.hat = Y.hat)\n",
    "  tau.hat <- predict(tau.forest,estimate.variance = TRUE,newdata = df_main)\n",
    "  summary(tau.hat$predictions)\n",
    "  sigma.hat <- sqrt(tau.hat$variance.estimates)\n",
    "  \n",
    "  pred = tau.hat$predictions\n",
    "  X5. =  tau.hat$predictions - 1.96 * sigma.hat\n",
    "  X95. = tau.hat$predictions + 1.96 * sigma.hat\n",
    "  \n",
    "  cate_CForest[df_main$ID,] <- as.data.frame(cbind(pred,X5.,X95.))\n",
    "  colnames(cate_CForest) <- c(\"pred\",\"X5.\",\"X95.\")\n",
    "  \n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "# Estimate CATE for each observation\n",
    "\n",
    "SL.nnet_td <- create.Learner(\"SL.nnet\", params = list(maxit=5000))\n",
    "SL.ranger_td = create.Learner(\"SL.ranger\", params = list(num.trees = 1000, min.node.size = 10))\n",
    "\n",
    "learners <- c(\"SL.ranger_1\",\"SL.xgboost\",\"SL.nnet_1\")\n",
    "\n",
    "\n",
    "score_T <- matrix(0,nrow(data),1)\n",
    "\n",
    "pseudo_all <- matrix(NA,nrow(data),5)\n",
    "ID_pseudo <- 1:nrow(data)\n",
    "pseudo_all <- cbind(pseudo_all,ID_pseudo)\n",
    "\n",
    "\n",
    "res_learner_p <- matrix(0,length(learners),5*2)\n",
    "res_learner_m1 <- matrix(0,length(learners),5*2)\n",
    "res_learner_m0 <- matrix(0,length(learners),5*2)\n",
    "res_learner_dr <- matrix(0,length(learners),5*2)\n",
    "res_learner_m <- matrix(0,length(learners),5*2)\n",
    "res_learner_R <- matrix(0,length(learners),5*2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### # 5-fold sample splitting\n",
    "# Sample splitting\n",
    "set.seed(1234)\n",
    "folds <- createFolds(data$d,k=5)\n",
    "\n",
    "\n",
    "for(f in 1:(length(folds))){\n",
    "  \n",
    "  if(f == 1){\n",
    "    data1 <- data[c(folds[[5]],folds[[2]],folds[[3]],folds[[4]]),]\n",
    "    df_main <- data[folds[[1]],]\n",
    "  } \n",
    "  if(f == 2){\n",
    "    data1 <- data[c(folds[[1]],folds[[5]],folds[[3]],folds[[4]]),]\n",
    "    df_main <- data[folds[[2]],]\n",
    "  } \n",
    "  \n",
    "  if(f == 3){\n",
    "    data1 <- data[c(folds[[1]],folds[[2]],folds[[5]],folds[[4]]),]\n",
    "    df_main <- data[folds[[3]],]\n",
    "  } \n",
    "  \n",
    "  if(f == 4){\n",
    "    data1 <- data[c(folds[[1]],folds[[2]],folds[[3]],folds[[5]]),]\n",
    "    df_main <- data[folds[[4]],]\n",
    "  } \n",
    "  \n",
    "  if(f == 5){\n",
    "    data1 <- data[c(folds[[1]],folds[[2]],folds[[3]],folds[[4]]),]\n",
    "    df_main <- data[folds[[5]],]\n",
    "  } \n",
    "  \n",
    "  df_aux <- data1\n",
    "  \n",
    "  \n",
    "  ## DR-learner \n",
    "  # Train a classification model to get the propensity scores\n",
    "  p_mod <- SuperLearner(Y = df_aux$d, X = df_aux[,covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                        verbose = FALSE, method = \"method.NNLS\", family = binomial(),cvControl = control)\n",
    "  \n",
    "  p_hat <- p_mod$SL.predict\n",
    "  p_hat = ifelse(p_hat<0.025, 0.025, ifelse(p_hat>.975,.975, p_hat)) # Overlap bounding\n",
    "  \n",
    "  \n",
    "  # Split the training data into treatment and control observations\n",
    "  aux_1 <- df_aux[which(df_aux$d==1),]\n",
    "  aux_0 <- df_aux[which(df_aux$d==0),]\n",
    "  \n",
    "  # Train a regression model for the treatment observations\n",
    "  m1_mod <- SuperLearner(Y = aux_1$y, X = aux_1[,covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                         verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "  \n",
    "  m1_hat <- m1_mod$SL.predict\n",
    "  \n",
    "  # Train a regression model for the control observations\n",
    "  m0_mod <- SuperLearner(Y = aux_0$y, X = aux_0[,covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                         verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "  \n",
    "  m0_hat <- m0_mod$SL.predict\n",
    "  \n",
    "  \n",
    "  \n",
    "  # Apply the doubly-robust estimator \n",
    "  y_mo <- (m1_hat - m0_hat) + ((df_main$d*(df_main$y -m1_hat))/p_hat) - ((1-df_main$d)*(df_main$y - m0_hat)/(1-p_hat))\n",
    "  \n",
    "  \n",
    "  \n",
    "  ## Collect all pseudo outcomes\n",
    "  pseudo_all[,1][df_main$ID] <- y_mo\n",
    "  pseudo_all[,5][df_main$ID] <- p_hat\n",
    "  \n",
    "  \n",
    "  score_T[,1][df_main$ID] = predict(m1_mod,df_main[,covariates])$pred - predict(m0_mod,df_main[,covariates])$pred\n",
    "  \n",
    "  \n",
    "  ######################\n",
    "  \n",
    "  \n",
    "  \n",
    "  ### R-learner \n",
    "  \n",
    "  # Train a regression model \n",
    "  m_mod <- SuperLearner(Y = df_aux$y, X = df_aux[,covariates], newX = df_main[,covariates], SL.library = learners,\n",
    "                        verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "  \n",
    "  m_hat <- m_mod$SL.predict\n",
    "  \n",
    "  # Apply the R-learner (residual-on-residual approach)\n",
    "  y_tilde = df_main$y - m_hat\n",
    "  w_tilde = df_main$d - p_hat\n",
    "  pseudo_outcome = y_tilde/w_tilde\n",
    "  \n",
    "  weights = w_tilde^2\n",
    "  ## Collect all pseudo outcomes\n",
    "  pseudo_all[,2][df_main$ID] <- pseudo_outcome\n",
    "  pseudo_all[,3][df_main$ID] <- weights\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  ###########\n",
    "  \n",
    "  \n",
    "  \n",
    "  ###  X-learner\n",
    "  \n",
    "  tau1 <- df_main[which(df_main$d==1),\"y\"] - m0_hat[which(df_main$d==1),]\n",
    "  \n",
    "  tau0 <- m1_hat[which(df_main$d==0),]  -  df_main[which(df_main$d==0),\"y\"]\n",
    "  \n",
    "  ## Collect all pseudo outcomes\n",
    "  pseudo_all[,4][ (df_main$ID[df_main$d==1])] <- tau1\n",
    "  pseudo_all[,4][ (df_main$ID[df_main$d==0])] <- tau0\n",
    "  \n",
    "  print(f)\n",
    "  \n",
    "  \n",
    "  a <- c(seq(1,2*length(folds),by=2))\n",
    "  a <- a[f]\n",
    "  res_learner_p[,a] <- p_mod$cvRisk\n",
    "  res_learner_p[,a+1] <- p_mod$coef\n",
    "  \n",
    "  res_learner_m1[,a] <- m1_mod$cvRisk\n",
    "  res_learner_m1[,a+1] <- m1_mod$coef\n",
    "  \n",
    "  res_learner_m0[,a] <- m0_mod$cvRisk\n",
    "  res_learner_m0[,a+1] <- m0_mod$coef\n",
    "  \n",
    "  \n",
    "  res_learner_m[,a] <- m_mod$cvRisk\n",
    "  res_learner_m[,a+1] <- m_mod$coef\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "# DR-learner final estimate  \n",
    "a  <- tryCatch({\n",
    "  dr_mod <- SuperLearner(Y = pseudo_all[,1], X = data[,covariates], newX = data[,covariates], SL.library = learners,\n",
    "                         verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "  \n",
    "  score_dr <- dr_mod$SL.predict\n",
    "  a <- score_dr\n",
    "  \n",
    "  \n",
    "},error=function(e){\n",
    "  \n",
    "  mean_score <- mean(pseudo_all[,1])\n",
    "  score_dr <- rep.int(mean_score, times = nrow(data))\n",
    "  a <- score_dr\n",
    "  return(a)\n",
    "})\n",
    "\n",
    "score_dr <- a\n",
    "\n",
    "res_learner_dr[,1] <- dr_mod$cvRisk\n",
    "res_learner_dr[,1+1] <- dr_mod$coef\n",
    "\n",
    "## Cross-fit version\n",
    "# Combined method\n",
    "res_combined_dr <- matrix(NA,nrow(data),5)\n",
    "# loop\n",
    "for(l in 1:10){\n",
    "  \n",
    "  set <- seq(from=1, to=nrow(data)+1,by=(nrow(data)/10))\n",
    "  set\n",
    "  \n",
    "  if(l<=5){\n",
    "    dr_mod_cf <- SuperLearner(Y = pseudo_all[(set[l]:set[l+1]-1),1],X=data[(set[l]:set[l+1]-1),covariates], \n",
    "                              newX = data[(set[6]:set[11]-1),covariates], SL.library = learners,\n",
    "                               verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "    \n",
    "    score_dr_1_cf <- dr_mod_cf$SL.predict\n",
    "    res_combined_dr[(set[6]:set[11]-1),l] <- score_dr_1_cf\n",
    "  }\n",
    "  if(l>5){\n",
    "    dr_mod_cf <- SuperLearner(Y =pseudo_all[(set[l]:set[l+1]-1),1],X=data[(set[l]:set[l+1]-1),covariates], \n",
    "                              newX =data[(set[1]:set[6]-1),covariates], SL.library = learners,\n",
    "                              verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "    \n",
    "    score_dr_0_cf <- dr_mod_cf$SL.predict\n",
    "    res_combined_dr[(set[1]:set[6]-1),(l-5)] <- score_dr_0_cf\n",
    "  }\n",
    "  \n",
    "}\n",
    "\n",
    "score_dr_oob <- rowMeans(res_combined_dr)\n",
    "\n",
    "#R-learner final estimate\n",
    "\n",
    "\n",
    "R_mod <- SuperLearner(Y = pseudo_all[,2], X = data[,covariates], newX = data[,covariates], SL.library = learners,\n",
    "                      verbose = FALSE, method = \"method.NNLS\",obsWeights = pseudo_all[,3],cvControl = control)\n",
    "score_R <- R_mod$SL.predict\n",
    "\n",
    "res_learner_R[,1] <- R_mod$cvRisk\n",
    "res_learner_R[,1+1] <- R_mod$coef\n",
    "\n",
    "\n",
    "\n",
    "# Cross-fit version\n",
    "res_combined_r <- matrix(NA,nrow(data),5)\n",
    "# loop\n",
    "for(l in 1:10){\n",
    "  \n",
    "  set <- seq(from=1, to=nrow(data)+1,by=(nrow(data)/10))\n",
    "  set\n",
    "  \n",
    "  if(l<=5){\n",
    "    r_mod_cf <- SuperLearner(Y = pseudo_all[(set[l]:set[l+1]-1),2],X=data[(set[l]:set[l+1]-1),covariates], \n",
    "                              newX = data[(set[6]:set[11]-1),covariates], SL.library = learners,\n",
    "                              verbose = FALSE, method = \"method.NNLS\",obsWeights = pseudo_all[(set[l]:set[l+1]-1),3],cvControl = control)\n",
    "    \n",
    "    score_r_1_cf <- r_mod_cf$SL.predict\n",
    "    res_combined_r[(set[6]:set[11]-1),l] <- score_r_1_cf\n",
    "  }\n",
    "  if(l>5){\n",
    "    r_mod_cf <- SuperLearner(Y =pseudo_all[(set[l]:set[l+1]-1),2],X=data[(set[l]:set[l+1]-1),covariates], \n",
    "                              newX =data[(set[1]:set[6]-1),covariates], SL.library = learners,\n",
    "                              verbose = FALSE, method = \"method.NNLS\",obsWeights = pseudo_all[(set[l]:set[l+1]-1),3],cvControl = control)\n",
    "    \n",
    "    score_r_0_cf <- r_mod_cf$SL.predict\n",
    "    res_combined_r[(set[1]:set[6]-1),(l-5)] <- score_r_0_cf\n",
    "  }\n",
    "  \n",
    "}\n",
    "\n",
    "score_R_oob <- rowMeans(res_combined_r)\n",
    "\n",
    "# X-learner final estimate\n",
    "a1  <- tryCatch({\n",
    "  tau1_mod <- SuperLearner(Y = pseudo_all[,4][data$d==1], X = data[which(data$d==1),covariates], newX = data[,covariates], SL.library = learners,\n",
    "                           verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "  \n",
    "  score_tau1 <- tau1_mod$SL.predict\n",
    "  a1 <- score_tau1\n",
    "  \n",
    "  \n",
    "},error=function(e){\n",
    "  \n",
    "  mean_score <- mean(pseudo_all[,4])\n",
    "  score_tau1 <- rep.int(mean_score, times = nrow(data))\n",
    "  a1 <- score_tau1\n",
    "  return(a1)\n",
    "})\n",
    "\n",
    "score_tau1 <- a1\n",
    "\n",
    "a0  <- tryCatch({\n",
    "  tau0_mod <- SuperLearner(Y =pseudo_all[,4][data$d==0], X = data[which(data$d==0),covariates], newX = data[,covariates], SL.library = learners,\n",
    "                           verbose = FALSE, method = \"method.NNLS\",cvControl = control)\n",
    "  \n",
    "  score_tau0 <- tau0_mod$SL.predict\n",
    "  a0 <- score_tau0\n",
    "  \n",
    "  \n",
    "},error=function(e){\n",
    "  \n",
    "  mean_score <- mean(pseudo_all[,5])\n",
    "  score_tau0 <- rep.int(mean_score, times = nrow(data))\n",
    "  a0 <- score_tau0\n",
    "  return(a0)\n",
    "})\n",
    "\n",
    "score_tau0 <- a0\n",
    "\n",
    "\n",
    "\n",
    "score_X <- pseudo_all[,5]*score_tau0 + (1-pseudo_all[,5])*score_tau1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cate_DR <- bootCI(pred_B=results_cate_DR,score=score_dr_oob)\n",
    "cate_R <- bootCI(pred_B=results_cate_R,score=score_R_oob)\n",
    "cate_T <- bootCI(pred_B=results_cate_T,score=score_T)\n",
    "cate_X <- bootCI(pred_B=results_cate_X,score=score_X)\n",
    "\n",
    "\n",
    "# Risk evaluation\n",
    "coef_nuisance <- c(rowMeans(res_learner_p[,c(2,4,6,8,10)]),rowMeans(res_learner_m0[,c(2,4,6,8,10)]),\n",
    "                   rowMeans(res_learner_m1[,c(2,4,6,8,10)]),rowMeans(res_learner_m[,c(2,4,6,8,10)]),\n",
    "                   res_learner_dr[,2],res_learner_R[,2])\n",
    "table_nuisance <- matrix(coef_nuisance,length(learners),6,byrow = F)\n",
    "rownames(table_nuisance) <- learners\n",
    "colnames(table_nuisance) <- c(\"prop-score\",\"mu0\",\"mu1\",\"mu\",\"DR\",\"R\")\n",
    "table_nuisance\n",
    "\n",
    "\n",
    "xtable(table_nuisance, digits=2)\n",
    "\n",
    "\n",
    "library(psych)\n",
    "z = cbind(cate_CBART$pred, cate_CForest$pred,score_dr_oob,score_R_oob,score_T,score_X)\n",
    "colnames(z) = c(\"Causal-BART\",\"Causal-Forest\",\"DR\",\"R\",\"T\",\"X\")\n",
    "pairs.panels(z, \n",
    "             method = \"pearson\", # correlation method\n",
    "             hist.col = \"blue\",\n",
    "             density = TRUE,  # show density plots\n",
    "             ellipses = TRUE, # show correlation ellipses\n",
    "             show.points = F)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare data for plot\n",
    "prep_plot <- function(cate){\n",
    "  cate1 <- melt(cate[order(cate$pred),])\n",
    "  cate1$ID <- rep(1:nrow(cate),times=3)\n",
    "  return(cate1)\n",
    "}\n",
    "\n",
    "# Define some colors\n",
    "cbp <- c(\"#000000\", \"#E69F00\", \"#0072B2\", \"#009E73\",\n",
    "         \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mm_DR <- prep_plot(cate_DR)\n",
    "mm_R <- prep_plot(cate_R)\n",
    "mm_T <- prep_plot(cate_T)\n",
    "mm_X <- prep_plot(cate_X)\n",
    "mm_CBART <- prep_plot(cate_CBART)\n",
    "mm_CF <- prep_plot(cate_CForest)\n",
    "\n",
    "mm <- rbind(mm_DR,mm_R,mm_T,mm_X,mm_CBART,mm_CF)\n",
    "mm$Method <- rep(c(\"DR-learner\",\"R-learner\",\"T-learner\",\"X-learner\",\"Causal-BART\",\"Causal-Forest\"),each=nrow(mm_DR))\n",
    "\n",
    "# Plot the CATE estimates + CI for 5% and 95%\n",
    "# Plot the CATE estimates + CI for 5% and 95%\n",
    "ggplot(mm, aes(x=ID,y=value,group=variable))+\n",
    "  geom_line(aes(color=variable,alpha=variable),size=1.1)+\n",
    "  scale_alpha_manual(values=c(1.0,0.05,0.05),labels = c(\"CATE\",\"CI lower 5%\", \"CI upper 95%\")) +\n",
    "  theme_cowplot() +\n",
    "  facet_wrap( ~ Method, scales=\"free\", nrow=4) + # Facet wrap with common scales \n",
    "  labs(y = \"Treatment Effect \", x = \"Ordered Observation\") +\n",
    "  theme(legend.position=\"none\", legend.justification = 'center')+ \n",
    "  scale_color_manual(values = cbp,labels = c(\"CATE\",\"CI lower 5%\", \"CI upper 95%\")) +\n",
    "  geom_smooth(data=mm[mm[\"variable\"] == \"X5.\" | mm[\"variable\"] == \"X95.\",],linetype=\"dashed\", size=0.5) +\n",
    "  ylim(-1000,5000)\n",
    "\n",
    "\n",
    "### Estimate 20% least, ATE and 80% most affected\n",
    "\n",
    "\n",
    "quantiles <- function(CATE){\n",
    "  S2        <- CATE+runif(length(CATE), 0, 0.00001) # Include white noise to guarantee that the score (S) differs from the baseline effect\n",
    "  breaks    <- quantile(S2, seq(0,1, 0.2),  include.lowest =T) # Quantiles create 5 groups by default - no parameter necessary \n",
    "  breaks[1] <- breaks[1] - 0.002 # Offset for lower tails \n",
    "  breaks[6] <- breaks[6] + 0.002 # Offset for upper tails\n",
    "  SG        <- cut(S2, breaks = breaks)\n",
    "  lev20 <- levels(SG)[1]\n",
    "  lev80 <- levels(SG)[5]\n",
    "  S_20 <- mean(CATE[SG==lev20])\n",
    "  S_ATE <- mean(CATE)\n",
    "  S_80 <- mean(CATE[SG==lev80])\n",
    "  \n",
    "  return(data.frame(S_20,S_ATE,S_80))\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "cate_list <- list(cate_DR$pred,cate_R$pred,cate_T$pred,cate_X$pred,cate_CBART$pred,cate_CForest$pred)\n",
    "res <- lapply(cate_list, quantiles)\n",
    "res\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586752a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a14df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
